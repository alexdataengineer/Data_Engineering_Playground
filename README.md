# 🚧 Data Engineering Playground

This repository is a **personal workspace** for exploring, prototyping, and building data engineering solutions using **Python**, **PySpark**, **Airflow**, and **dbt**.

It contains structured subfolders with notebooks, dbt models, DAGs, and local development infrastructure for testing and learning.

---

## 📁 Folder Structure

├── dags/ # Apache Airflow DAGs
├── docker-compose/ # Docker environment for local orchestration
├── models/ # dbt models (ELT logic)
├── notebooks/ # Jupyter/Databricks notebooks for EDA & pipeline prototyping
├── stg/ # General staging code and raw transformations
├── .user.yml # dbt user config
├── dbt_project.yml # dbt project configuration
├── profiles.yml # dbt profiles (connection setup)
└── README.md # Project documentation


---

## 🧰 Tech Stack

- **Python 3.x**
- **Apache Spark (PySpark)**
- **Apache Airflow**
- **dbt (Data Build Tool)**
- **Docker / Docker Compose**
- Jupyter Notebooks

---

## ✨ Highlights

- End-to-end data transformation flows.
- Modular and testable DAGs with Airflow.
- Local development with `docker-compose`.
- dbt models for transforming raw/staged data.
- PySpark notebooks for handling large-scale data.

---

## 🚧 Disclaimer

This repo is under active development and serves as a testing ground. Some scripts and notebooks may be experimental or incomplete.

---

## 🤝 Contributions

This is a personal project, but feel free to fork or explore if you find something useful. If you have questions or suggestions, feel free to open an issue or reach out.

