# Scraper de Emails de Cl√≠nicas e Consult√≥rios M√©dicos

Este projeto cont√©m scripts Python para extrair emails de cl√≠nicas, consult√≥rios e profissionais de sa√∫de de diferentes fontes na web.

## üìã √çndice

- [Instala√ß√£o](#instala√ß√£o)
- [Scripts Dispon√≠veis](#scripts-dispon√≠veis)
- [Como Usar](#como-usar)
- [Configura√ß√£o](#configura√ß√£o)
- [Funcionalidades](#funcionalidades)
- [Avisos Legais](#avisos-legais)
- [Troubleshooting](#troubleshooting)

## üöÄ Instala√ß√£o

### 1. Instalar Depend√™ncias

```bash
cd scraping
pip install -r requirements.txt
```

### 2. Instalar ChromeDriver (para Selenium)

Para usar o Selenium, voc√™ precisa do ChromeDriver:

**macOS:**
```bash
brew install chromedriver
```

**Ubuntu/Debian:**
```bash
sudo apt-get install chromium-chromedriver
```

**Windows:**
Baixe de: https://chromedriver.chromium.org/

## üìÅ Scripts Dispon√≠veis

### 1. `clinic_email_scraper.py` - Scraper Principal
- **Funcionalidade**: Scraping completo com Selenium e requests
- **Recursos**: Busca por cidade, extra√ß√£o de URLs, valida√ß√£o de emails
- **Uso**: Para extra√ß√µes em larga escala

### 2. `simple_clinic_scraper.py` - Scraper Simples
- **Funcionalidade**: Vers√£o mais direta e r√°pida
- **Recursos**: Foco em sites de diret√≥rio m√©dico
- **Uso**: Para extra√ß√µes r√°pidas e simples

### 3. `advanced_clinic_scraper.py` - Scraper Avan√ßado
- **Funcionalidade**: M√∫ltiplas fontes e estrat√©gias
- **Recursos**: Valida√ß√£o robusta, m√∫ltiplos formatos de sa√≠da
- **Uso**: Para extra√ß√µes profissionais

## üéØ Como Usar

### Execu√ß√£o B√°sica

```bash
# Scraper principal
python clinic_email_scraper.py

# Scraper simples
python simple_clinic_scraper.py

# Scraper avan√ßado
python advanced_clinic_scraper.py
```

### Execu√ß√£o Personalizada

```python
from clinic_email_scraper import ClinicEmailScraper

# Criar inst√¢ncia
scraper = ClinicEmailScraper()

# Definir cidades espec√≠ficas
cities = [
    ("S√£o Paulo", "SP"),
    ("Rio de Janeiro", "RJ"),
    ("Belo Horizonte", "MG"),
]

# Executar scraping
results = scraper.run_scraping(cities, max_pages_per_city=3)

# Salvar resultados
filename = scraper.save_results()
```

## ‚öôÔ∏è Configura√ß√£o

### Arquivo `config.py`

Voc√™ pode personalizar as configura√ß√µes editando o arquivo `config.py`:

```python
# Adicionar mais cidades
CITIES = [
    ("Sua Cidade", "UF"),
    # ... outras cidades
]

# Adicionar especialidades m√©dicas
MEDICAL_SPECIALTIES = [
    "sua_especialidade",
    # ... outras especialidades
]

# Configurar delays
SETTINGS = {
    'delay_between_requests': (2, 5),  # Delay maior
    'max_pages_per_city': 10,  # Mais p√°ginas por cidade
}
```

### Configura√ß√µes Importantes

- **`delay_between_requests`**: Delay entre requisi√ß√µes para evitar bloqueios
- **`max_pages_per_city`**: N√∫mero m√°ximo de p√°ginas por cidade
- **`timeout`**: Timeout para requisi√ß√µes HTTP
- **`headless`**: Executar Selenium em modo headless (sem interface gr√°fica)

## üîß Funcionalidades

### ‚úÖ Recursos Implementados

- **Extra√ß√£o de Emails**: Regex robusto para encontrar emails v√°lidos
- **Valida√ß√£o de Conte√∫do**: Filtra apenas p√°ginas relacionadas √† medicina
- **M√∫ltiplas Fontes**: Diret√≥rios m√©dicos, busca Google, sites espec√≠ficos
- **Rota√ß√£o de User Agents**: Evita detec√ß√£o de bot
- **Delays Aleat√≥rios**: Comportamento mais humano
- **Logs Detalhados**: Acompanhamento do progresso
- **M√∫ltiplos Formatos**: Excel, CSV, logs
- **Tratamento de Erros**: Continua mesmo com falhas
- **Deduplica√ß√£o**: Remove emails duplicados

### üéØ Estrat√©gias de Busca

1. **Diret√≥rios M√©dicos**: Sites como Doctoralia, MedicinaNet
2. **Busca por Cidade**: Termos espec√≠ficos por localiza√ß√£o
3. **Especialidades**: Busca por especialidades m√©dicas
4. **Sites Espec√≠ficos**: URLs conhecidas de cl√≠nicas

### üìä Output

Os resultados s√£o salvos em:

- **Excel** (`.xlsx`): Formato principal com todas as informa√ß√µes
- **CSV** (`.csv`): Formato simples para an√°lise
- **Logs** (`.log`): Registro detalhado da execu√ß√£o
- **Estat√≠sticas** (`.txt`): Resumo dos resultados

## ‚ö†Ô∏è Avisos Legais

### Importante!

1. **Respeite os Termos de Servi√ßo**: Verifique os termos de cada site antes do scraping
2. **Rate Limiting**: Use delays apropriados para n√£o sobrecarregar servidores
3. **Robots.txt**: Respeite o arquivo robots.txt dos sites
4. **Uso Respons√°vel**: Use os dados apenas para fins leg√≠timos
5. **LGPD**: Respeite a Lei Geral de Prote√ß√£o de Dados

### Recomenda√ß√µes

- Use delays de pelo menos 1-3 segundos entre requisi√ß√µes
- Limite o n√∫mero de p√°ginas por site
- Monitore os logs para detectar bloqueios
- Use proxies se necess√°rio (configur√°vel)

## üîç Troubleshooting

### Problemas Comuns

#### 1. Erro de ChromeDriver
```
WebDriverException: Message: unknown error: cannot find Chrome binary
```

**Solu√ß√£o:**
```bash
# Instalar Chrome
# macOS
brew install google-chrome

# Ubuntu
sudo apt-get install google-chrome-stable
```

#### 2. Timeout nas Requisi√ß√µes
```
requests.exceptions.Timeout
```

**Solu√ß√£o:**
- Aumentar o timeout no `config.py`
- Verificar conex√£o com internet
- Usar delays maiores

#### 3. Poucos Resultados
**Poss√≠veis Causas:**
- Sites bloqueando requisi√ß√µes
- Padr√µes de email muito restritivos
- Palavras-chave muito espec√≠ficas

**Solu√ß√µes:**
- Usar User Agents diferentes
- Ajustar padr√µes de email
- Adicionar mais termos de busca

#### 4. Erro de Depend√™ncias
```
ModuleNotFoundError: No module named 'selenium'
```

**Solu√ß√£o:**
```bash
pip install -r requirements.txt
```

### Logs e Debug

Os scripts geram logs detalhados:

```bash
# Ver logs em tempo real
tail -f scraping.log

# Ver logs avan√ßados
tail -f advanced_scraping.log
```

## üìà Exemplos de Uso

### Exemplo 1: Busca R√°pida
```bash
python simple_clinic_scraper.py
```

### Exemplo 2: Busca Completa
```bash
python clinic_email_scraper.py
```

### Exemplo 3: Busca Personalizada
```python
from advanced_clinic_scraper import AdvancedClinicScraper

scraper = AdvancedClinicScraper()

# Buscar apenas em S√£o Paulo
scraper.search_google_clinics("S√£o Paulo", "SP")

# Salvar resultados
scraper.save_results("sao_paulo_clinicas.xlsx")
```

## ü§ù Contribui√ß√£o

Para contribuir com o projeto:

1. Fork o reposit√≥rio
2. Crie uma branch para sua feature
3. Commit suas mudan√ßas
4. Push para a branch
5. Abra um Pull Request

## üìû Suporte

Se encontrar problemas:

1. Verifique os logs de erro
2. Consulte a se√ß√£o Troubleshooting
3. Verifique se todas as depend√™ncias est√£o instaladas
4. Teste com configura√ß√µes mais conservadoras

## üìÑ Licen√ßa

Este projeto √© para uso educacional e de pesquisa. Use responsavelmente e respeite os termos de servi√ßo dos sites acessados. 